<h2>Results of the PARSEME shared task - edition 1.1 (2018)</h2>

<p><small>Last updated: May 11, 2018</small></p>


<!----------------------->
<h1>Participants</h1>

<p>For the 2018 edition of the PARSEME shared task, <strong>12 teams</strong> submitted <strong>17 system results</strong> to the shared task: 13 to the closed track and 4 to the open track. No team submitted results for the 20 languages of the shared task, but 11 teams submitted systems covering 19 languages (all except Arabic).
In the tables below, systems are referred to by anonymous nicknames. Authors of the systems and their affiliations will be made public after reviewing system description papers.</p>

<!----------------------->
<h1>Results description</h1>

<p>The evaluation results are given below. First, we present rankings for each language separately. Second, we presend cross-lingual rankings averaged over languages, both for the general metrics and for metrics focusing on specific phenomena (discontinuity, variability, etc.) Third, we present non-ranked results of systems per language and per VMWE category.</p>

<ol id="top-menu">
  <li><a href="lang">Language-specific system rankings</a>
  <ul>
    <li>
      <a href="lang-BG">BG</a> |
      <a href="lang-DE">DE</a> |
      <a href="lang-EL">EL</a> |
      <a href="lang-EN">EN</a> |
      <a href="lang-ES">ES</a> |
      <a href="lang-EU">EU</a> |
      <a href="lang-FA">FA</a> |
      <a href="lang-FR">FR</a> |
      <a href="lang-HE">HE</a> |
      <a href="lang-HI">HI</a> |
      <a href="lang-HR">HR</a> |
      <a href="lang-HU">HU</a> |
      <a href="lang-IT">IT</a> |
      <a href="lang-LT">LT</a> |
      <a href="lang-PL">PL</a> |
      <a href="lang-PT">PT</a> |
      <a href="lang-RO">RO</a> |
      <a href="lang-SL">SL</a> |
      <a href="lang-TR">TR</a>
    </li>
  </ul>
  </li> 
  <li><a href="avg">Cross-lingual macro-averages</a>
  <ul>
    <li>
      <a href="avg-general">General ranking</a>
    </li>  
    <li>Continuity: 
      <a href="avg-Continuous">Continuous VMWEs</a> |
      <a href="avg-Discontinuous">Discontinuous VMWEs</a> 
    </li>
    <li>Number of tokens: 
      <a href="avg-Multi-token">Multi-token VMWEs</a> |
      <a href="avg-Single-token">Single-token VMWEs</a> 
    </li>
    <li>Novelty: 
      <a href="avg-Seen-in-train">Seen-in-train VMWEs</a> |
      <a href="avg-Unseen-in-train">Unseen-in-train VMWEs</a> 
    </li>
    <li>Variability: 
      <a href="avg-Variant-of-train">Variant-of-train VMWEs</a> |
      <a href="avg-Identical-to-train">Identical-to-train VMWEs</a> 
    </li>
  </ul>
  </li>   
  <li><a href="cat">Results per VMWE category (not ranked)</a>
  <ul>
    <li>
      <a href="cat-BG">BG</a> |
      <a href="cat-DE">DE</a> |
      <a href="cat-EL">EL</a> |
      <a href="cat-EN">EN</a> |
      <a href="cat-ES">ES</a> |
      <a href="cat-EU">EU</a> |
      <a href="cat-FA">FA</a> |
      <a href="cat-FR">FR</a> |
      <a href="cat-HE">HE</a> |
      <a href="cat-HI">HI</a> |
      <a href="cat-HR">HR</a> |
      <a href="cat-HU">HU</a> |
      <a href="cat-IT">IT</a> |
      <a href="cat-LT">LT</a> |
      <a href="cat-PL">PL</a> |
      <a href="cat-PT">PT</a> |
      <a href="cat-RO">RO</a> |
      <a href="cat-SL">SL</a> |
      <a href="cat-TR">TR</a>
    </li>
  </ul>
  </li>  
</ol>

<p>Open-track and closed-track systems are shown in the same table, separated by a thick line. We report MWE-based and Token-based precision (P), recall (R) and F-measure (F1), as described on the <a href="?sitesig=CONF&page=CONF_04_LAW-MWE-CxG_2018&subpage=CONF_50_Evaluation_metrics">evaluation metrics page</a>. For macro-averages, languages for which a system did not submit any result are considered as having P=R=F1=0. Macro-averaged rankings indicate the number of languages for which the system provided results. For phenomenon-specific macro-averaged rankings, only MWE-based measures are reported, and languages not containing a given phenomenon are ignored (only relevant for Single-token VMWEs).</p>

<p>Remember that the aim of this shared task is not to rank systems/participants, but to encourage a cross-lingual discussion on VMWE identification models. The <a href="?sitesig=CONF&page=CONF_04_LAW-MWE-CxG_2018&subpage=CONF_40_Shared_Task#participation">participation policy</a> may have introduced bias to the annotated corpora and the participating systems. Therefore, the ranking results should be considered with <strong>caution</strong>.</p>


